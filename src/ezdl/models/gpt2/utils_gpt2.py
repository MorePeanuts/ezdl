import torch
import torch.nn.functional as F
from deprecated import deprecated
from ...tokenizer.tokenizer_utils import Tokenizer


@deprecated(
    version="1.0.0",
    reason="This function will be replaced by `generate` method of GenerateMixin class in the future..",
)
def generate_text_simple(
    model, input_ids, max_new_tokens, context_length
) -> torch.Tensor:
    """
    Generate text using a simple greedy sampling approach.

    Args:
        model (GPT2ModelForCausalLM): The GPT-2 model to use for text generation.
        input_ids (torch.Tensor): The input token IDs.
        max_new_tokens (int): The maximum number of new tokens to generate.
        context_length (int): The length of the context window.

    Returns:
        torch.Tensor: The generated token IDs.
    """
    model.eval()
    for _ in range(max_new_tokens):
        context_ids = input_ids[:, -context_length:]
        with torch.no_grad():
            logits = model(context_ids)
        logits = logits[:, -1, :]  # Last token generated by model.

        # The following steps are redundant and are only used to
        # demonstrate the principle.
        probas = torch.softmax(logits, dim=-1)
        idx_next = torch.argmax(probas, dim=-1, keepdim=True)
        input_ids = torch.cat((input_ids, idx_next), dim=1)

    return input_ids


def text_to_token_ids(text, tokenizer: Tokenizer, allowed_special={"<|endoftext|>"}):
    """
    Convert text to token IDs using the provided tokenizer.

    Args:
        text (str): The input text.
        tokenizer (Tokenizer): The tokenizer to use for conversion.
        allowed_special (set): Set of allowed special tokens.

    Returns:
        torch.Tensor: The token IDs.
    """
    encoded_text = tokenizer.encode(text, allowed_special=allowed_special)
    encoded_tensor = torch.tensor(encoded_text).unsqueeze(0) # add batch dimension
    return encoded_tensor

    
def token_ids_to_text(token_ids, tokenizer: Tokenizer):
    """
    Convert token IDs to text using the provided tokenizer.

    Args:
        token_ids (torch.Tensor): The input token IDs.
        tokenizer (Tokenizer): The tokenizer to use for conversion.

    Returns:
        str: The decoded text.
    """
    flat = token_ids.squeeze(0)
    return tokenizer.decode(flat.tolist())
    
    
def calc_loss_batch(input_batch, target_batch, model, device, last_token_only=False):
    """
    Calculate the loss for a batch of input and target data.

    Args:
        input_batch (torch.Tensor): The input data.
        target_batch (torch.Tensor): The target data.
        model (torch.nn.Module): The model to use for prediction.
        device (torch.device): The device to use for computation.
        last_token_only (bool): Whether to calculate loss only for the last token. Defaults to False.

    Returns:
        torch.Tensor: The loss value.
    """
    input_batch, target_batch = input_batch.to(device), target_batch.to(device)
    logits = model(input_batch)
    if last_token_only:
        logits = logits[:, -1, :]
        loss = F.cross_entropy(logits, target_batch)
    else:
        loss = F.cross_entropy(logits.flatten(0, 1), target_batch.flatten())
    return loss


def calc_loss_dataloader(dataloader, model, device, num_batches=None, last_token_only=False):
    """
    Calculate the loss for a dataloader.

    Args:
        dataloader (torch.utils.data.DataLoader): The dataloader to use for data.
        model (torch.nn.Module): The model to use for prediction.
        device (torch.device): The device to use for computation.
        num_batches (int, optional): The number of batches to use for calculation. Defaults to None.
        last_token_only (bool, optional): Whether to use only the last token for loss calculation. Defaults to False.

    Returns:
        float: The average loss value.
    """
    total_loss = 0.
    if len(dataloader) == 0:
        return float("nan")
    elif num_batches is None:
        num_batches = len(dataloader)
    else:
        num_batches = min(num_batches, len(dataloader))
    for i, (input_batch, target_batch) in enumerate(dataloader):
        if i < num_batches:
            loss = calc_loss_batch(input_batch, target_batch, model, device, last_token_only)
            total_loss += loss.item()
        else:
            break
    return total_loss / num_batches
    
    
def calc_accuracy_dataloader(dataloader, model, device, num_batches=None):
    """
    Calculate the accuracy for a dataloader.

    Args:
        dataloader (torch.utils.data.DataLoader): The dataloader to use for data.
        model (torch.nn.Module): The model to use for prediction.
        device (torch.device): The device to use for computation.
        num_batches (int, optional): The number of batches to use for calculation. Defaults to None.

    Returns:
        float: The average accuracy value.
    """
    model.eval()
    correct_predictions, num_samples = 0, 0
    
    if num_batches is None:
        num_batches = len(dataloader)
    else:
        num_batches = min(num_batches, len(dataloader))
        
    for i, (input_batch, target_batch) in enumerate(dataloader):
        if i < num_batches:
            input_batch, target_batch = input_batch.to(device), target_batch.to(device)
            
            with torch.no_grad():
                logits = model(input_batch)[:, -1]
            predictions = torch.argmax(logits, dim=-1)
            num_samples += predictions.size(0)
            correct_predictions += (predictions == target_batch).sum().item()
        else:
            break 
    return correct_predictions / num_samples
