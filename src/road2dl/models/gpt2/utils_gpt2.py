import torch
import torch.nn.functional as F
from deprecated import deprecated
from ...tokenizer import Tokenizer


@deprecated(
    version="1.0.0",
    reason="This function will be replaced by `generate` method of GenerateMixin class in the future..",
)
def generate_text_simple(
    model, input_ids, max_new_tokens, context_length
) -> torch.Tensor:
    """
    Generate text using a simple greedy sampling approach.

    Args:
        model (GPT2ModelForCausalLM): The GPT-2 model to use for text generation.
        input_ids (torch.Tensor): The input token IDs.
        max_new_tokens (int): The maximum number of new tokens to generate.
        context_length (int): The length of the context window.

    Returns:
        torch.Tensor: The generated token IDs.
    """
    model.eval()
    for _ in range(max_new_tokens):
        context_ids = input_ids[:, -context_length:]
        with torch.no_grad():
            logits = model(context_ids)
        logits = logits[:, -1, :]  # Last token generated by model.

        # The following steps are redundant and are only used to
        # demonstrate the principle.
        probas = torch.softmax(logits, dim=-1)
        idx_next = torch.argmax(probas, dim=-1, keepdim=True)
        input_ids = torch.cat((input_ids, idx_next), dim=1)

    return input_ids


def text_to_token_ids(text, tokenizer: Tokenizer, allowed_special={"<|endoftext|>"}):
    """
    Convert text to token IDs using the provided tokenizer.

    Args:
        text (str): The input text.
        tokenizer (Tokenizer): The tokenizer to use for conversion.
        allowed_special (set): Set of allowed special tokens.

    Returns:
        torch.Tensor: The token IDs.
    """
    encoded_text = tokenizer.encode(text, allowed_special=allowed_special)
    encoded_tensor = torch.tensor(encoded_text).unsqueeze(0) # add batch dimension
    return encoded_tensor

    
def token_ids_to_text(token_ids, tokenizer: Tokenizer):
    """
    Convert token IDs to text using the provided tokenizer.

    Args:
        token_ids (torch.Tensor): The input token IDs.
        tokenizer (Tokenizer): The tokenizer to use for conversion.

    Returns:
        str: The decoded text.
    """
    flat = token_ids.squeeze(0)
    return tokenizer.decode(flat.tolist())
    
    
def calc_loss_batch(input_batch, target_batch, model, device):
    input_batch, target_batch = input_batch.to(device), target_batch.to(device)
    logits = model(input_batch)
    loss = F.cross_entropy(logits.flatten(0, 1), target_batch.flatten())
    return loss


def calc_loss_dataloader(dataloader, model, device, num_batches=None):
    total_loss = 0.
    if len(dataloader) == 0:
        return float("nan")
    elif num_batches is None:
        num_batches = len(dataloader)
    else:
        num_batches = min(num_batches, len(dataloader))
    for i, (input_batch, target_batch) in enumerate(dataloader):
        if i < num_batches:
            loss = calc_loss_batch(input_batch, target_batch, model, device)
            total_loss += loss.item()
        else:
            break
    return total_loss / num_batches