{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "566fa15d-6820-43ed-9bae-3d79ce5b4660",
   "metadata": {},
   "source": [
    "以the-verdict.txt为例展示手动分词，先统计总字符数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68c86fed-ea84-4012-9160-b6fa9e59aee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 20479\n"
     ]
    }
   ],
   "source": [
    "with open('../../datasets/the_verdict/the-verdict.txt', 'r', encoding='utf-8') as f:\n",
    "    raw_text = f.read()\n",
    "print(\"Total number of character:\", len(raw_text)) # 20479"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7432ee-5590-4953-b732-b45a769f4a65",
   "metadata": {},
   "source": [
    "简单分词："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a6633dd-a340-4598-b979-d530632b5491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens (from scratch): 4690\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def tokenize_scratch(text: str) -> list[str]:\n",
    "    result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "    result = [item for item in result if item.strip()]\n",
    "    return result\n",
    "\n",
    "tokenized_text = tokenize_scratch(raw_text)\n",
    "print(\"Total number of tokens (from scratch):\", len(tokenized_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4784458-9620-4e5d-8b95-261e4fe0d748",
   "metadata": {},
   "source": [
    "将token转换为tokenID，首先需要构建一个词表："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "520c5efe-084d-4302-9ab8-1cbc71d57b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique tokens (vocab size): 1130\n"
     ]
    }
   ],
   "source": [
    "def build_vocab_scratch(tokenized_text: list[str]) -> tuple[dict[str, int], int]:\n",
    "    all_tokens = sorted(set(tokenized_text))\n",
    "    vocab_size = len(all_tokens)\n",
    "    vocab = {token: idx for idx, token in enumerate(all_tokens)}\n",
    "    return vocab, vocab_size\n",
    "\n",
    "vocab, vocab_size = build_vocab_scratch(tokenized_text)\n",
    "print(\"Total number of unique tokens (vocab size):\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb64445-01e0-4e58-8b06-4376e07d2be3",
   "metadata": {},
   "source": [
    "创建一个逆向词表（inverse vocabulary），将 token ID 映射回对应的词元"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6680708a-0a3e-4f5b-85ff-ee14681cda18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
     ]
    }
   ],
   "source": [
    "class SimpleTokenizer:\n",
    "    def __init__(self, vocab: dict[str, int]):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {idx: token for token, idx in vocab.items()}\n",
    "        \n",
    "    def encode(self, text: str) -> list[int]:\n",
    "        tokenized_text = tokenize_scratch(text)\n",
    "        ids = [self.str_to_int[token] for token in tokenized_text]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids: list[int]) -> str:\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text\n",
    "    \n",
    "tokenizer = SimpleTokenizer(vocab)\n",
    "text = \"\"\"\"It's the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463b1d1f-3ed3-4744-904d-2356ee4634c3",
   "metadata": {},
   "source": [
    "新增支持 <|unk|> 和 <|endoftext|> 两个特殊词元"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0b84088-fdfc-42b4-85ea-66225d71e89d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique tokens (vocab size): 1132\n",
      "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]\n",
      "<|unk>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk>.\n"
     ]
    }
   ],
   "source": [
    "def build_vocab_scratch(tokenized_text: list[str]) -> tuple[dict[str, int], int]:\n",
    "    all_tokens = sorted(list(set(tokenized_text)))\n",
    "    all_tokens.extend([\"<|endoftext|>\", \"<|unk>\"])\n",
    "    vocab_size = len(all_tokens)\n",
    "    vocab = {token: idx for idx, token in enumerate(all_tokens)}\n",
    "    return vocab, vocab_size\n",
    "\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self, vocab: dict[str, int]):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {idx: token for token, idx in vocab.items()}\n",
    "        \n",
    "    def encode(self, text: str) -> list[int]:\n",
    "        tokenized_text = tokenize_scratch(text)\n",
    "        tokenized_text = [token if token in self.str_to_int else \"<|unk>\" for token in tokenized_text]\n",
    "        ids = [self.str_to_int[token] for token in tokenized_text]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids: list[int]) -> str:\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text\n",
    "    \n",
    "vocab, vocab_size = build_vocab_scratch(tokenized_text)\n",
    "print(\"Total number of unique tokens (vocab size):\", vocab_size)\n",
    "tokenizer = SimpleTokenizer(vocab)\n",
    "\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)\n",
    "print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7403f7-6c71-4659-a24b-2e1f968a2e4a",
   "metadata": {},
   "source": [
    "GPT使用字节对编码（BPE）的方式。\n",
    "\n",
    "BPE 算法的实现相对复杂，我们可以使用 tiktoken 库，该库基于 Rust 源代码高效实现了 BPE 算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fff720b6-851b-4535-ae8e-d58ff7626317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 286, 617, 34680, 27271, 13]\n",
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "text = \"Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\" \n",
    "ids = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"}) \n",
    "print(ids)\n",
    "print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa710bf1-16a8-4dfc-91cc-4f2cfab6872d",
   "metadata": {},
   "source": [
    "下面我们实现一个数据加载器（data loader），通过滑动窗口（sliding window）方法从训练数据集中获取输入-目标对，首先对全文进行分词："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4158f1e9-c20f-48c4-81c5-0fdc4536562f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens (with gpt2 tokenizer): 5145\n"
     ]
    }
   ],
   "source": [
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(\"Total number of tokens (with gpt2 tokenizer):\", len(enc_text)) # 5145"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecdc49a-67b1-4a8b-aedb-5edb783f72a0",
   "metadata": {},
   "source": [
    "BPE 分词器的 encode 方法会一次性完成分词和 token ID 转换两个步骤。下面实现数据加载器："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92584839-0d2d-49cc-9dcf-ff2137748a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPT2Dataset(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        \n",
    "        token_ids = tokenizer.encode(txt)\n",
    "        \n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i : i + max_length]\n",
    "            target_chunk = token_ids[i + 1 : i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "    \n",
    "def create_GPT2_dataloader(txt, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPT2Dataset(txt, tokenizer, max_length, stride)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47f3e81-fd0e-48b9-9a76-59b313b19c8a",
   "metadata": {},
   "source": [
    "为 LLM 训练准备输入文本的最后一步，是将 token ID 转化为嵌入向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e774cb2-d4ce-4af5-91e3-7980af1e1a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([2, 3, 5, 1])\n",
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "print(embedding_layer.weight.data)\n",
    "print(embedding_layer(input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e58c382-8479-47ce-8ca6-9a7618ab6da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50257\n",
    "hidden_dim = 256\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, hidden_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae38d89a-78a5-476f-b564-a5d214e0e2c3",
   "metadata": {},
   "source": [
    "将每个批次中的每个词元转化为一个 256 维嵌入向量。假设批次大小为 8 且每个样本包含 4 个词元，最终将生成一个 8×4×256 的三维张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb6cf76-5cfb-43d3-a9c4-6e93bcc4d29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 4\n",
    "dataloader = create_GPT2_dataloader(\n",
    "    raw_text, batch_size=8, max_length=max_length, \n",
    "    stride=max_length, shuffle=False\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs shape:\\n\", inputs.shape)\n",
    "\n",
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(\"Embedding inputs shape:\\n\", token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88803060-4fea-4d26-bc5e-16e3f37c4504",
   "metadata": {},
   "source": [
    "对于 GPT 模型的绝对嵌入方法，只需创建另一个与 token_embedding_layer 维度相同的嵌入层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39d3bf0-f2ad-429e-845a-15d6ff20e442",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_embedding_layer = torch.nn.Embedding(max_length, hidden_dim)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
    "print(\"Position embedding shape:\\n\", pos_embeddings.shape) # (4, 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a297f04-f5d4-4979-8ce9-8a168743e8a7",
   "metadata": {},
   "source": [
    "直接将位置嵌入编码与词元嵌入相加，得到最终输入到 LLM 中的嵌入向量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aac5a9f-ccb3-42c4-a3fa-dddbbe23bed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(\"Input embeddings shape:\\n\", input_embeddings.shape) # (8, 4, 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560e799f-367b-4a0b-8a99-fa4ebff6db88",
   "metadata": {},
   "source": [
    "最后介绍字节对编码（BPE）的实现。\n",
    "\n",
    "先引入字节的概念。考虑将文本转换为字节数组："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dab430-04c5-49ad-8c4b-7aa4288318ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is some text\"\n",
    "byte_ary = bytearray(text, \"utf-8\")\n",
    "print(byte_ary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b229182-c286-439f-8b54-1eeae0d169c3",
   "metadata": {},
   "source": [
    "对 bytearray 对象调用 list()时，每个字节会被视为单独的元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7636f68a-91c9-4f4e-920c-438c126b9c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = list(byte_ary)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbce96df-1837-4068-97e8-66fe804bca17",
   "metadata": {},
   "source": [
    "BPE 分词器有一个词汇表，其中每个 token ID 对应的是完整单词或子词，而非单个字符。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e13871-50de-4c87-bae7-bb46635285bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "gpt2_tokenizer.encode(\"This is some text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4144fa-b295-4906-91bd-b47950cc34c1",
   "metadata": {},
   "source": [
    "由于一个字节由 8 位 bit 组成，因此单个字节可以表示$2^8=256$种可能的值，范围从 0 到 255。BPE 分词器通常将这 256 个值作为其前 256 个单字符词元"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c318092-a4e3-4d52-a773-b95e7e20915c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "for i in range(300):\n",
    "    decoded = gpt2_tokenizer.decode([i])\n",
    "    print(f\"{i}: {decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527a9876-2ef6-48a8-bede-a3ba388faad9",
   "metadata": {},
   "source": [
    "BPE 分词算法的目标是构建一个由高频子词（如 298: ent，该子词可见于 entangle、entertain、enter、entrance、entity 等单词）甚至完整单词组成的词表"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537c049b-5a3c-409f-8c0e-42383b49739a",
   "metadata": {},
   "source": [
    "以下是上述算法的 Python 类实现，它模仿了 tiktoken Python 接口："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f68eaa9-659d-4d12-98fb-a62d11346b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPETokenizerSimple:\n",
    "    \"\"\"\n",
    "    Simple BPE tokenizer implementation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Maps token_id to token_str (e.g., {11246: \"some\"})\n",
    "        self.vocab: dict[int, str] = {}\n",
    "        # Maps token_str to token_id (e.g., {\"some\": 11246})\n",
    "        self.inverse_vocab: dict[str, int] = {}\n",
    "        # Dictionary of BPE merges: {(token_id1, token_id2): merged_token_id}\n",
    "        self.bpe_merges: dict[tuple[int, int], int] = {}\n",
    "        # For the official OpenAI GPT-2 merges, use a rank dict:\n",
    "        # of form {(string_A, string_B): rank}, where lower rank = higher priority\n",
    "        self.bpe_ranks: dict[tuple[str, str], int] = {}\n",
    "\n",
    "    def train(self, text: str, vocab_size: int, allowed_special: set[str] = {\"<|endoftext|>\"}):\n",
    "        \"\"\"\n",
    "        Train the BPE tokenizer on the given text from scratch.\n",
    "        \n",
    "        Args:\n",
    "            text (str): The text to train the tokenizer on.\n",
    "            vocab_size (int): The desired vocabulary size.\n",
    "            allowed_special (set): A set of special tokens to allow in the vocabulary.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Preprocess: Replace spaces with \"Ġ\"\n",
    "        # Note that Ġ is a particularity of the GPT-2 BPE implementation\n",
    "        # E.g., \"Hello world\" might be tokenized as [\"Hello\", \"Ġworld\"]\n",
    "        # (GPT-4 BPE would tokenize it as [\"Hello\", \" world\"])\n",
    "        processed_text = []\n",
    "        for i, ch in enumerate(text):\n",
    "            if ch == \" \" and i != 0:\n",
    "                processed_text.append(\"Ġ\")\n",
    "            if ch != \" \":\n",
    "                processed_text.append(ch)\n",
    "        processed_text = \"\".join(processed_text)\n",
    "        \n",
    "        # Initialize vocab with unique characters, including \"Ġ\" if present\n",
    "        # Start with the first 256 ASCII characters\n",
    "        unique_chars = [chr(i) for i in range(256)]\n",
    "        unique_chars.extend(\n",
    "            ch for ch in sorted(set(processed_text))\n",
    "            if ch not in unique_chars\n",
    "        )\n",
    "        if \"Ġ\" not in unique_chars:\n",
    "            unique_chars.append(\"Ġ\")\n",
    "        self.vocab = {i: ch for i, ch in enumerate(unique_chars)}\n",
    "        self.inverse_vocab = {ch: i for i, ch in self.vocab.items()}\n",
    "        \n",
    "        # Add allowed special tokens\n",
    "        for token in allowed_special:\n",
    "            if token not in self.inverse_vocab:\n",
    "                new_id = len(self.vocab)\n",
    "                self.vocab[new_id] = token\n",
    "                self.inverse_vocab[token] = new_id\n",
    "                \n",
    "        # Tokenize the processed_text into token IDs\n",
    "        token_ids = [self.inverse_vocab[ch] for ch in processed_text]\n",
    "        \n",
    "        # BPE steps 1-3: Repeatedly find and replace frequent pairs\n",
    "        for new_id in range(len(self.vocab), vocab_size):\n",
    "            pair_id = self.find_freq_pair(token_ids, mode=\"most\")\n",
    "            if pair_id is None:\n",
    "                break\n",
    "            token_ids = self.replace_pair(token_ids, pair_id, new_id)\n",
    "            self.bpe_merges[pair_id] = new_id\n",
    "            \n",
    "        # Build the vocabulary with merged tokens\n",
    "        for (p0, p1), new_id in self.bpe_merges.items():\n",
    "            merged_token = self.vocab[p0] + self.vocab[p1]\n",
    "            self.vocab[new_id] = merged_token\n",
    "            self.inverse_vocab[merged_token] = new_id\n",
    "            \n",
    "    @staticmethod\n",
    "    def find_freq_pair(token_ids: list[int], mode=\"most\"):\n",
    "        pairs = Counter(zip(token_ids[:-1], token_ids[1:]))\n",
    "        if not pairs:\n",
    "            return None\n",
    "        match mode:\n",
    "            case \"most\":\n",
    "                # return max(pairs.items(), key=lambda x: x[1])[0]\n",
    "                return pairs.most_common(1)[0][0]\n",
    "            case \"least\":\n",
    "                # return min(pairs.items(), key=lambda x: x[1])[0]\n",
    "                return pairs.most_common()[-1][0]\n",
    "            case _:\n",
    "                raise ValueError(\"Invalid mode. Choose 'most' or 'least'\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def replace_pair(token_ids: list[int], pair_id: tuple[int, int], new_id: int):\n",
    "        dq = deque(token_ids)\n",
    "        replaced: list[int] = []\n",
    "        \n",
    "        while dq:\n",
    "            current = dq.popleft()\n",
    "            if dq and (current, dq[0]) == pair_id:\n",
    "                replaced.append(new_id)\n",
    "                dq.popleft()\n",
    "            else:\n",
    "                replaced.append(current)\n",
    "        \n",
    "        return replaced\n",
    "\n",
    "    def load_vocab_and_merges_from_gpt2(self, vocab_path, bpe_merges_path):\n",
    "        \"\"\"\n",
    "        Load pre-trained vocabulary and BPE merges from OpenAI's GPT-2 files.\n",
    "        \n",
    "        Args:\n",
    "            vocab_path (str): Path to the vocab file (GPT-2 calls it 'encoder.json' or 'vocab.json').\n",
    "            bpe_merges_path (str): Path to the bpe_merges file  (GPT-2 calls it 'vocab.bpe' or 'merges.txt').\n",
    "        \"\"\"\n",
    "        \n",
    "        # Load vocabulary\n",
    "        with open(vocab_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            loaded_vocab = json.load(file)\n",
    "            # Convert loaded vocabulary to correct format\n",
    "            self.vocab = {int(v): k for k, v in loaded_vocab.items()}\n",
    "            self.inverse_vocab = {k: int(v) for k, v in loaded_vocab.items()}\n",
    "\n",
    "        # Handle newline character without adding a new token\n",
    "        if \"\\n\" not in self.inverse_vocab:\n",
    "            # Use an existing token ID as a placeholder for '\\n'\n",
    "            # Preferentially use \"<|endoftext|>\" if available\n",
    "            fallback_token = next(\n",
    "                (token for token in [\"<|endoftext|>\", \"Ġ\", \"\"] if token in self.inverse_vocab), None\n",
    "            )\n",
    "            if fallback_token is not None:\n",
    "                newline_token_id = self.inverse_vocab[fallback_token]\n",
    "            else:\n",
    "                # If no fallback token is available, raise an error\n",
    "                raise KeyError(\"No suitable token found in vocabulary to map '\\\\n'.\")\n",
    "            self.inverse_vocab[\"\\n\"] = newline_token_id\n",
    "            self.vocab[newline_token_id] = \"\\n\"\n",
    "            \n",
    "        # Load GPT-2 merges and store them with an assigned \"rank\"\n",
    "        self.bpe_ranks = {}  # reset ranks\n",
    "        with open(bpe_merges_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            lines = file.readlines()\n",
    "            if lines and lines[0].startswith(\"#\"):\n",
    "                lines = lines[1:]\n",
    "            rank = 0\n",
    "            for line in lines:\n",
    "                pair = tuple(line.strip().split())\n",
    "                if len(pair) == 2:\n",
    "                    token1, token2 = pair\n",
    "                    # If token1 or token2 not in vocab, skip\n",
    "                    if token1 in self.inverse_vocab and token2 in self.inverse_vocab:\n",
    "                        self.bpe_ranks[(token1, token2)] = rank\n",
    "                        rank += 1\n",
    "                    else:\n",
    "                        print(f\"Skipping pair {pair} as one token is not in the vocabulary.\")\n",
    "\n",
    "    def encode(self, text: str, allowed_special: set[str] | None = None):\n",
    "        \"\"\"\n",
    "        Encode the input text into a list of token IDs, with tiktoken-style handling of special tokens.\n",
    "    \n",
    "        Args:\n",
    "            text (str): The input text to encode.\n",
    "            allowed_special (set or None): Special tokens to allow passthrough. If None, special handling is disabled.\n",
    "    \n",
    "        Returns:\n",
    "            List of token IDs.\n",
    "        \"\"\"\n",
    "\n",
    "        token_ids: list[int] = []\n",
    "        \n",
    "        # If special token handling is enabled\n",
    "        if allowed_special is not None and len(allowed_special) > 0:\n",
    "            # Build regex to match allowed special tokens\n",
    "            special_pattern = (\n",
    "                \"(\" + \"|\".join(re.escape(tok) for tok in sorted(allowed_special, key=len, reverse=True)) + \")\"\n",
    "            )\n",
    "            # Encoding text while handling special tokens\n",
    "            last_index = 0\n",
    "            for mch in re.finditer(special_pattern, text):\n",
    "                prefix = text[last_index : mch.start()]\n",
    "                # Encode prefix without special handling\n",
    "                token_ids.extend(self.encode(prefix, allowed_special=None))  \n",
    "                # Encode special token\n",
    "                special_token = mch.group(0)\n",
    "                if special_token in self.inverse_vocab:\n",
    "                    token_ids.append(self.inverse_vocab[special_token])\n",
    "                else:\n",
    "                    raise ValueError(f\"Special token {special_token} not found in vocabulary.\")\n",
    "                last_index = mch.end()\n",
    "            # Remaining part to process normally\n",
    "            text = text[last_index:]  \n",
    "            \n",
    "        # If no special tokens, or remaining text after special token split:\n",
    "        tokens: list[str] = []\n",
    "        lines = text.split(\"\\n\")\n",
    "        for i, line in enumerate(lines):\n",
    "            if i > 0:\n",
    "                tokens.append(\"\\n\")\n",
    "            words = line.split()\n",
    "            for j, word in enumerate(words):\n",
    "                if j == 0 and i > 0:\n",
    "                    tokens.append(\"Ġ\" + word)\n",
    "                elif j == 0:\n",
    "                    tokens.append(word)\n",
    "                else:\n",
    "                    tokens.append(\"Ġ\" + word)\n",
    "\n",
    "        for token in tokens:\n",
    "            if token in self.inverse_vocab:\n",
    "                token_ids.append(self.inverse_vocab[token])\n",
    "            else:\n",
    "                token_ids.extend(self.tokenize_with_bpe(token))\n",
    "    \n",
    "        return token_ids\n",
    "        \n",
    "    def tokenize_with_bpe(self, token: str):\n",
    "        \"\"\"\n",
    "        Tokenize a single token which is not in vocabulary using BPE merges.\n",
    "\n",
    "        Args:\n",
    "            token (str): The token to tokenize.\n",
    "\n",
    "        Returns:\n",
    "            list[int]: The list of token IDs after applying BPE.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Tokenize the token into individual characters (as initial token IDs)\n",
    "        raw_token_ids = [self.inverse_vocab.get(ch, None) for ch in token]\n",
    "        token_ids = [tid for tid in raw_token_ids if tid is not None]\n",
    "        if None in raw_token_ids:\n",
    "            missing_chars = [ch for ch, tid in zip(token, raw_token_ids) if tid is None]\n",
    "            raise ValueError(f\"Characters not found in vocab: {missing_chars}\")\n",
    "            \n",
    "        # If we haven't loaded OpenAI's GPT-2 merges, use my approach\n",
    "        if not self.bpe_ranks:\n",
    "            can_merge = True\n",
    "            while can_merge and len(token_ids) > 1:\n",
    "                can_merge = False\n",
    "                new_tokens: list[int] = []\n",
    "                i = 0\n",
    "                while i < len(token_ids) - 1:\n",
    "                    pair = (token_ids[i], token_ids[i + 1])\n",
    "                    if pair in self.bpe_merges:\n",
    "                        merged_token_id = self.bpe_merges[pair]\n",
    "                        new_tokens.append(merged_token_id)\n",
    "                        # Merged pair {pair} -> {merged_token_id} ('{self.vocab[merged_token_id]}')\n",
    "                        i += 2  # Skip the next token as it's merged\n",
    "                        can_merge = True\n",
    "                    else:\n",
    "                        new_tokens.append(token_ids[i])\n",
    "                        i += 1\n",
    "                if i < len(token_ids):\n",
    "                    new_tokens.append(token_ids[i])\n",
    "                token_ids = new_tokens\n",
    "            return token_ids\n",
    "            \n",
    "        # Otherwise, do GPT-2-style merging with the ranks:\n",
    "        # 1) Convert token_ids back to string \"symbols\" for each ID\n",
    "        symbols = [self.vocab[id_num] for id_num in token_ids]\n",
    "\n",
    "        # Repeatedly merge all occurrences of the lowest-rank pair\n",
    "        while True:\n",
    "            # Collect all adjacent pairs\n",
    "            pairs = set(zip(symbols[:-1], symbols[1:]))\n",
    "            if not pairs:\n",
    "                break\n",
    "\n",
    "            # Find the pair with the lowest rank\n",
    "            bigram = min(pairs, key=lambda x: self.bpe_ranks.get(x, float('inf')))\n",
    "\n",
    "            # If no valid ranked pair is present, we're done\n",
    "            if bigram is None or bigram not in self.bpe_ranks:\n",
    "                break\n",
    "            # else merge all occurrences of that pair\n",
    "            first, second = bigram\n",
    "            new_symbols: list[str] = []\n",
    "            i = 0\n",
    "            while i < len(symbols):\n",
    "                # If we see (first, second) at position i, merge them\n",
    "                if i < len(symbols) - 1 and symbols[i] == first and symbols[i + 1] == second:\n",
    "                    new_symbols.append(first + second)  # merged symbol\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_symbols.append(symbols[i])\n",
    "                    i += 1\n",
    "            symbols = new_symbols\n",
    "            if len(symbols) == 1:\n",
    "                break\n",
    "\n",
    "        # Finally, convert merged symbols back to IDs\n",
    "        merged_ids = [self.inverse_vocab[sym] for sym in symbols]\n",
    "        return merged_ids\n",
    "\n",
    "    def decode(self, token_ids):\n",
    "        \"\"\"\n",
    "        Decode a list of token IDs back into a string.\n",
    "\n",
    "        Args:\n",
    "            token_ids (list[int]): The list of token IDs to decode.\n",
    "\n",
    "        Returns:\n",
    "            str: The decoded string.\n",
    "        \"\"\"\n",
    "        buffer: list[str] = []\n",
    "        for i, token_id in enumerate(token_ids):\n",
    "            if token_id not in self.vocab:\n",
    "                raise ValueError(f\"Token ID {token_id} not found in vocab.\")\n",
    "            token = self.vocab[token_id]\n",
    "            if token == \"\\n\":\n",
    "                if buffer and not buffer[-1].endswith(\" \"):\n",
    "                    buffer.append(\" \\n\") # Add space if not present before a newline\n",
    "            elif token.startswith(\"Ġ\"):\n",
    "                buffer.append(\" \" + token[1:])\n",
    "            else:\n",
    "                buffer.append(token)\n",
    "        return \"\".join(buffer)\n",
    "        \n",
    "    def save_vocab_and_merges(self, vocab_path, bpe_merges_path):\n",
    "        \"\"\"\n",
    "        Save the vocabulary and BPE merges to JSON files.\n",
    "\n",
    "        Args:\n",
    "            vocab_path (str): Path to save the vocabulary.\n",
    "            bpe_merges_path (str): Path to save the BPE merges.\n",
    "        \"\"\"\n",
    "        # Save vocabulary\n",
    "        with open(vocab_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            json.dump(self.vocab, file, ensure_ascii=False, indent=2)\n",
    "\n",
    "        # Save BPE merges as a list of dictionaries\n",
    "        with open(bpe_merges_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            merges_list = [{\"pair\": list(pair), \"new_id\": new_id}\n",
    "                           for pair, new_id in self.bpe_merges.items()]\n",
    "            json.dump(merges_list, file, ensure_ascii=False, indent=2)\n",
    "            \n",
    "    def load_vocab_and_merges(self, vocab_path, bpe_merges_path):\n",
    "        \"\"\"\n",
    "        Load the vocabulary and BPE merges from JSON files.\n",
    "\n",
    "        Args:\n",
    "            vocab_path (str): Path to the vocabulary file.\n",
    "            bpe_merges_path (str): Path to the BPE merges file.\n",
    "        \"\"\"\n",
    "        # Load vocabulary\n",
    "        with open(vocab_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            loaded_vocab = json.load(file)\n",
    "            self.vocab = {int(k): v for k, v in loaded_vocab.items()}\n",
    "            self.inverse_vocab = {v: int(k) for k, v in loaded_vocab.items()}\n",
    "\n",
    "        # Load BPE merges\n",
    "        with open(bpe_merges_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            merges_list = json.load(file)\n",
    "            for merge in merges_list:\n",
    "                pair = tuple(merge[\"pair\"])\n",
    "                new_id = merge[\"new_id\"]\n",
    "                self.bpe_merges[pair] = new_id\n",
    "                \n",
    "    @lru_cache(maxsize=None)\n",
    "    def get_special_token_id(self, token):\n",
    "        \"\"\"\n",
    "        Get the ID of a special token.\n",
    "\n",
    "        Args:\n",
    "            token (str): The special token.\n",
    "\n",
    "        Returns:\n",
    "            int or None: The ID of the token if it exists, otherwise None.\n",
    "        \"\"\"\n",
    "        return self.inverse_vocab.get(token, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaff6b7d-cab7-4eb6-9842-9a46f8185dbf",
   "metadata": {},
   "source": [
    "使用上面这个BPE分词器："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bbb34c-1b74-4c0f-a18c-0d29be7f10be",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BPETokenizerSimple()\n",
    "tokenizer.train(raw_text, vocab_size=1000, allowed_special={\"<|endoftext|>\"})\n",
    "print(len(tokenizer.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb3ee78-6387-49dd-9bf7-c14eb26d312e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tokenizer.bpe_merges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33884816-9d8c-4190-8854-15e7f9a8321c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Jack embraced beauty through art and life.\"\n",
    "token_ids = tokenizer.encode(input_text, allowed_special={\"<|endoftext|>\"})\n",
    "print(token_ids)\n",
    "print(tokenizer.decode(token_ids) == input_text)\n",
    "\n",
    "input_text = \"Jack embraced beauty through art and life.<|endoftext|>\"\n",
    "token_ids = tokenizer.encode(input_text, allowed_special={\"<|endoftext|>\"})\n",
    "print(token_ids)\n",
    "print(tokenizer.decode(token_ids) == input_text)\n",
    "\n",
    "input_text = \"Jack embraced bea 74uty through art and life.<|endoftext|>\"\n",
    "token_ids = tokenizer.encode(input_text, allowed_special={\"<|endoftext|>\"})\n",
    "print(token_ids)\n",
    "print(tokenizer.decode(token_ids) == input_text)\n",
    "\n",
    "print(\"Number of characters:\", len(input_text))\n",
    "print(\"Number of token IDs:\", len(token_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d5963d-eca2-4bce-88a0-655fd4a2cff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(token_ids)\n",
    "print(tokenizer.decode(token_ids))\n",
    "\n",
    "for token_id in token_ids:\n",
    "    print(f\"{token_id} -> {tokenizer.decode([token_id])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac1e073-f304-4630-8662-ad5ae862ea68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained tokenizer\n",
    "tokenizer.save_vocab_and_merges(\n",
    "    vocab_path=\"../../datasets/the_verdict/bpe_vocab.json\", bpe_merges_path=\"../../datasets/the_verdict/bpe_merges.txt\")\n",
    "# Load tokenizer\n",
    "tokenizer2 = BPETokenizerSimple()\n",
    "tokenizer2.load_vocab_and_merges(\n",
    "    vocab_path=\"../../datasets/the_verdict/bpe_vocab.json\", bpe_merges_path=\"../../datasets/the_verdict/bpe_merges.txt\")\n",
    "\n",
    "print(tokenizer2.decode(token_ids))\n",
    "tokenizer2.decode(\n",
    "    tokenizer2.encode(\"This is some text with \\n newline characters.\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46385b88-001b-4998-8057-2fe74d6f989a",
   "metadata": {},
   "source": [
    "加载gpt2的词表："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f02ebf1-4494-4795-9bd7-5199036b4408",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_1 = BPETokenizerSimple()\n",
    "gpt2_dir = \"../../models/gpt2/\"\n",
    "tokenizer_1.load_vocab_and_merges_from_gpt2(gpt2_dir+'vocab.json', gpt2_dir+'merges.txt')\n",
    "tokenizer_2 = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e9e44e-748c-4ebd-89c1-0d16cea50295",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Jack embraced beauty through art and life.<|endoftext|>\"\n",
    "token_ids = tokenizer_1.encode(input_text, allowed_special={\"<|endoftext|>\"})\n",
    "print(token_ids)\n",
    "decoded_text = tokenizer_1.decode(token_ids)\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4bf69b-31e0-4c43-a0b6-ab0387e85d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"    Jack embraced    \"\n",
    "token_ids = tokenizer_2.encode(input_text, allowed_special={\"<|endoftext|>\"})\n",
    "print(token_ids)\n",
    "decoded_text = tokenizer_2.decode(token_ids)\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84660919-9fc4-4dbd-9a9c-ed7754d066d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Jack embraced beauty through art and life.<|endoftext|>\"\n",
    "token_ids = tokenizer_2.encode(input_text, allowed_special={\"<|endoftext|>\"})\n",
    "print(token_ids)\n",
    "decoded_text = tokenizer_2.decode(token_ids)\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92f12f8-9555-46c0-8d4f-7a532af73206",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"     Jack embraced     \"\n",
    "token_ids = tokenizer_1.encode(input_text, allowed_special={\"<|endoftext|>\"})\n",
    "print(token_ids)\n",
    "decoded_text = tokenizer_1.decode(token_ids)\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b5822d-c3cc-40d9-a013-320bfe5633d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"     Jack embraced     \"\n",
    "token_ids = tokenizer_2.encode(input_text, allowed_special={\"<|endoftext|>\"})\n",
    "print(token_ids)\n",
    "decoded_text = tokenizer_2.decode(token_ids)\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bc947d-0093-4549-92df-e38338430aec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
